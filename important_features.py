import pandas as pd
from tqdm import tqdm
import numpy as np
from pandas import option_context
import json

def __gini(array):
    '''
    args: array - an array of numbers
    
    return: g_coeff - float that gives gini coefficient
    
    method: calculate the Gini coefficient of a numpy array
    '''
    
    array = array[array!=0]
    if len(array) == 0:
        return 0
    array = array.flatten()
    array = np.sort(array)
    index = np.arange(1, array.shape[0] + 1)
    n = array.shape[0]
    g_coeff = (np.sum((2 * index - n - 1) * array)) / (n * np.sum(array))
    
    return (g_coeff)

def __my_describe(num_series):
    '''
    args: num_series - Series; a series of numbers corresponding to each feature
    
    returns: num_desc - Dictionart; a dictionary of aggregated values (aggregate term as key)
    
    method:
    For the given set of numbers, provide
    1. mean
    2. median
    3. std (if there is only one datapoint, then DAAR and DCAT NaN, as std(NaN) is nan, replacing it with 0.0)
    4. gini coefficient
    5. IQR
    '''
    
    num_desc = {}
    num_desc['mean'] = num_series.mean() #1
    num_desc['median'] = num_series.quantile(q=0.5) #2
    num_desc['std'] = num_series.std() #3
    if np.isnan(num_desc['std']):
        num_desc['std'] = 0.0
    num_desc['gini'] = __gini(num_series.dropna().to_numpy()) #4
    Q1 = num_series.quantile(q=0.25)
    Q3 = num_series.quantile(q=0.75)
    num_desc['IQR'] = Q3 - Q1 #5
    
    return(num_desc)

def __aggregate(df_loc):
    '''
    args: df_loc - dataframe of activities for a contributor
    
    return: agg_characteristics - dictionary with charateristics name as key
    
    method:
    calculate mean, median, std, Gini and IQR for
    1. DCA: Time difference between consecutive activities
    2. NAR: Number of activities per repository
    3. NTR: Number of activity type per repository
    4. NCAR: Number of consecutive activities performed in each repository before switching to the next repository
    5. DCAR: Time spent in each repository before switching to the next repository
    6. DAAR: Time taken to switch from one repository to another
    7. DCAT: Time taken to switch from one activity type to another
    8. NAT: num of activities made per activity type
    '''
    
    agg_characteristics = {}
    
    #1
    time_diff_consec_act = (
                 df_loc[['date','activity','repository']]
                 .sort_values('date')
                 .assign(next_time_stamp = lambda d: d.date.shift(-1))
                 .assign(time_diff = lambda d: d.next_time_stamp - d.date)
                 .assign(time_diff = lambda d: d.time_diff/pd.to_timedelta('1 hour'))
                 ['time_diff']
                 .dropna()
               )
    time_desc = __my_describe(time_diff_consec_act)
    
    #2
    act_per_repo = (
                     df_loc
                     .groupby('repository')
                     .agg(act_count=('activity','count'))
                     ['act_count']
                    )
    act_per_repo_desc = __my_describe(act_per_repo)

    #3
    act_type_per_repo = (
                         df_loc
                         .groupby('repository')
                         .agg(act_type_count=('activity','nunique'))
                         ['act_type_count']
                        )
    act_type_per_repo_desc = __my_describe(act_type_per_repo)
    
    #4, #5, #6
    continuous_act_repo = (
                           df_loc
                           .sort_values('date')
                           .assign(group=lambda d: (d.repository != d.repository.shift(1)).cumsum())
                           .groupby('group')
                           .agg(activities=('activity', 'count'), first_time=('date','first'), last_time=('date','last'))
                           .assign(next_first_time=lambda d: d.first_time.shift(-1))
                           .assign(time_spent=lambda d: (d.last_time - d.first_time)/pd.to_timedelta('1 hour'))
                           .assign(time_to_switch=lambda d: (d.next_first_time - d.last_time)/pd.to_timedelta('1 hour'))
                          )
    #4
    cont_act_repo_counter_desc = __my_describe(continuous_act_repo['activities'])
    #5
    time_spent_in_repo_desc = __my_describe(continuous_act_repo['time_spent'])
    #6 
    time_to_switch_repo_desc = __my_describe(continuous_act_repo['time_to_switch'])#.dropna())
    
    #7
    continous_act_type = (
                          df_loc
                          .sort_values('date')
                          .assign(group=lambda d: (d.activity != d.activity.shift(1)).cumsum())
                          .groupby('group')
                          .agg(activities=('repository', 'count'), first_time=('date','first'), last_time=('date','last'))
                          .assign(next_first_time=lambda d: d.first_time.shift(-1))
                          .assign(time_spent=lambda d: (d.last_time - d.first_time)/pd.to_timedelta('1 hour'))
                          .assign(time_to_switch=lambda d: (d.next_first_time - d.last_time)/pd.to_timedelta('1 hour'))
                         )
    
    time_to_switch_act_type_desc = __my_describe(continous_act_type['time_to_switch'])#.dropna())
    
    #8
    act_per_act_type = (
                        df_loc
                        .groupby('activity')
                        .agg(act_count=('repository','count'))
                        ['act_count']
                       )
    act_per_act_type_desc = __my_describe(act_per_act_type)
    
    agg_characteristics['DCA'] = time_desc #1
    agg_characteristics['NAR'] = act_per_repo_desc #2
    agg_characteristics['NTR'] = act_type_per_repo_desc #3
    agg_characteristics['NCAR'] = cont_act_repo_counter_desc #4
    agg_characteristics['DCAR'] = time_spent_in_repo_desc #5
    agg_characteristics['DAAR'] = time_to_switch_repo_desc #6
    agg_characteristics['DCAT'] = time_to_switch_act_type_desc #7
    agg_characteristics['NAT'] = act_per_act_type_desc #8
    
    return(agg_characteristics)

def __stats(df_loc):
    '''
    args: df_loc - dataframe of activities for a contributor
    
    return: characteristics - dictionary with the charateristics name as key
    
    method:
    counting metrics:
    1. NA: number of activities
    2. NT: number of activity types
    3. NOR: number of owners involved with
    4. ORR: ratio of number of owners to repos
    
    aggregated metrics:
    invoke aggregate function to calculate the aggregation values for aggregated metrics
    '''

    # Raise assertion error if activities corresponding to more than one contributor is given
    unq_contribs = df_loc.contributor.unique()
    assert len(unq_contribs)==1, f'Provide activity details for one contributor at a time. The provided contributors are {unq_contribs}'
    
    individ_characteristics = {} # to store the characteristics that are represented as a single number
    characteristics = {} # to store the characteristics that are represented as a single number
    
    num_activities = np.int64(df_loc.groupby('contributor', as_index=False).count().rename(columns={'activity':'activities'})['activities'][0])
    num_activity_type = np.int64(df_loc.activity.nunique())
    num_owner = np.int64(df_loc.owner.nunique())
    ratio_owner_repo = np.float64(df_loc.owner.nunique()/df_loc.repository.nunique())
    
    individ_characteristics['NA'] = num_activities #1
    individ_characteristics['NT'] = num_activity_type #2
    individ_characteristics['NOR'] = num_owner #3
    individ_characteristics['ORR'] = ratio_owner_repo #4
    
    characteristics['feat'] = individ_characteristics
    characteristics.update(__aggregate(df_loc))
    
    return(characteristics)

def __convert_col_type(df_loc):    
    '''
    args: df_loc - DataFrame; a dataframe of all the features and their values for all the contributors
    
    returns: df_loc - DataFrame; a dataframe of all the features and their values (converted to required type) for all the contributors
    
    method: converts all the column to float then converts specific columns to int
    '''
    int_feat_col = ['feat_NA', 'feat_NT', 'feat_NOR']

    df_loc = df_loc.astype('float').round(3)
    for int_cols in int_feat_col:
        df_loc = df_loc.astype({int_cols:'int'})
    
    return(df_loc)

def extract_features(df):
    '''
    args: df - activity file name
    
    return: df_feat - dataframe of behavioural features for the given contributor
    
    method:
    get the characteristic features for the given contributor that is used to identify if they are bot or human.
    This function computes, 38 such features, namely NA: number of activities, 
    NT: number of activity types, NOR: number of owners of repositories,
    ORR: Owner repository ratio, 
    DCA: time difference between consecutive activities (mean, median, std and gini), 
    NAR: number of activities per repository (mean, median, gini and IQR),
    NTR: number of activity types per repository (mean, median, std and gini),
    NCAR: number of continuous activities in a repo (mean, std and IQR),
    DCAR: total time taken to perform consecutive activities in a repo (mean, median, std and IQR),
    DAAR: time taken to switch repos (mean, median, std, gini and IQR),
    DCAT: time taken to switch activity type (mean, median, std, gini and IQR),
    NAT: number of activities per type (mean, median, std, gini and IQR).
    '''

    df['date'] = pd.to_datetime(df.date, errors='coerce', format='%Y-%m-%dT%H:%M:%S+00:00').dt.tz_localize(None)
    df[['owner','repo']]=df.repository.str.split('/', expand=True)
    
    # active_contributors = list(df_active_contib_activities.contributor.unique())
    # all_act = list(df_active_contib_activities['activity'].unique())
    # for cont in active_contributors:
    
    # df_feat = (
    #         pd.DataFrame.from_dict(__stats(df),orient='index')
    #         .stack()
    #         .to_frame()
    #         .transpose()
    #         .set_index([[0]])
    # )
    df_feat = pd.json_normalize(__stats(df), sep='_')
    # with option_context('display.max_columns',None):
        # print(df_feat)

    # df_feat.columns = ['_'.join(col) for col in df_feat.columns.values]
    important_features = ['NA','NT','NOR','ORR',
                          'DCA_mean','DCA_median','DCA_std','DCA_gini',
                          'NAR_mean','NAR_median','NAR_gini','NAR_IQR',
                          'NTR_mean','NTR_median','NTR_std','NTR_gini',
                          'NCAR_mean','NCAR_std','NCAR_IQR',
                          'DCAR_mean','DCAR_median','DCAR_std','DCAR_IQR',
                          'DAAR_mean','DAAR_median','DAAR_std','DAAR_gini','DAAR_IQR',
                          'DCAT_mean','DCAT_median','DCAT_std','DCAT_gini','DCAT_IQR',
                          'NAT_mean','NAT_median','NAT_std','NAT_gini','NAT_IQR']
    df_feat = (
        __convert_col_type(df_feat)
        .rename(columns={'feat_NA':'NA', 'feat_NT':'NT', 'feat_NOR':'NOR', 'feat_ORR':'ORR'})
        [important_features]
        .sort_index()
    )
    
    return(df_feat)